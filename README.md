# bedrockx

<div align="center">

[![PyPI version](https://badge.fury.io/py/bedrockx.svg)](https://pypi.org/project/bedrockx/)
[![Python Version](https://img.shields.io/pypi/pyversions/bedrockx.svg)](https://pypi.org/project/bedrockx/)
[![License](https://img.shields.io/github/license/ciaoyizhen/bedrockx.svg)](https://github.com/ciaoyizhen/bedrockx/blob/main/LICENSE)
[![Downloads](https://pepy.tech/badge/bedrockx)](https://pepy.tech/project/bedrockx)

**ä¸€ä¸ªå¼ºå¤§çš„ Python å·¥å…·åº“ï¼Œè®©æ•°æ®å¤„ç†å˜å¾—ç®€å•é«˜æ•ˆ**

[å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹) â€¢ [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§) â€¢ [æ–‡æ¡£](#è¯¦ç»†æ–‡æ¡£) â€¢ [ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹) â€¢ [è´¡çŒ®](#è´¡çŒ®)

</div>

---

## ğŸ“– ç®€ä»‹

bedrockx æ˜¯ä¸€ä¸ªä¸“ä¸ºæ•°æ®å¤„ç†å·¥ä½œæµè®¾è®¡çš„ Python å·¥å…·åº“ï¼Œæä¾›äº†æ–‡ä»¶æ“ä½œã€æ•°æ®å¤„ç†ã€å¤šçº¿ç¨‹åŠ é€Ÿç­‰å¸¸ç”¨åŠŸèƒ½ã€‚æ— è®ºæ‚¨æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¿˜æ˜¯éœ€è¦å¿«é€Ÿå®ç°æ•°æ® ETLï¼Œbedrockx éƒ½èƒ½æ˜¾è‘—æå‡æ‚¨çš„å·¥ä½œæ•ˆç‡ã€‚

### âœ¨ ä¸ºä»€ä¹ˆé€‰æ‹© bedrockxï¼Ÿ

- ğŸš€ **ç®€å•æ˜“ç”¨**ï¼šç»Ÿä¸€çš„ API è®¾è®¡ï¼Œä¸€è¡Œä»£ç å®Œæˆå¤æ‚æ“ä½œ
- ğŸ“ **å¤šæ ¼å¼æ”¯æŒ**ï¼šæ”¯æŒ JSONã€JSONLã€CSVã€Excel ç­‰å¤šç§æ•°æ®æ ¼å¼
- âš¡ **é«˜æ€§èƒ½**ï¼šå†…ç½®å¤šçº¿ç¨‹å¤„ç†ï¼Œè½»æ¾åº”å¯¹å¤§è§„æ¨¡æ•°æ®
- ğŸ”§ **çµæ´»å¯æ‰©å±•**ï¼šæä¾›åŸºç±»å’Œè£…é¥°å™¨ï¼Œæ–¹ä¾¿è‡ªå®šä¹‰æ‰©å±•
- ğŸ“ **å®Œå–„çš„æ—¥å¿—**ï¼šé›†æˆæ—¥å¿—ç®¡ç†ï¼Œæ–¹ä¾¿è°ƒè¯•å’Œç›‘æ§
- ğŸ§ª **æµ‹è¯•è¦†ç›–**ï¼šå®Œæ•´çš„å•å…ƒæµ‹è¯•ï¼Œä¿è¯ä»£ç è´¨é‡

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install bedrockx
```

### 5 åˆ†é’Ÿä¸Šæ‰‹

```python
from bedrockx import read_file, save_file, filter_data

# 1. è¯»å–æ•°æ®ï¼ˆè‡ªåŠ¨è¯†åˆ«æ ¼å¼ï¼‰
data = read_file("data.jsonl")

# 2. å¤„ç†æ•°æ®
filtered = filter_data(data, filter_set={1, 2}, main_key_column="id")

# 3. ä¿å­˜ç»“æœ
save_file("output.json", filtered)
```

å°±æ˜¯è¿™ä¹ˆç®€å•ï¼ğŸ‰

---

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

### ğŸ“‚ æ–‡ä»¶æ“ä½œ

#### ç»Ÿä¸€çš„æ–‡ä»¶è¯»å–æ¥å£

æ”¯æŒå¤šç§æ ¼å¼ï¼ˆJSONã€JSONLã€CSVã€Excelï¼‰å’Œå¤šç§è¾“å‡ºç±»å‹ï¼ˆlistã€dictã€setï¼‰ï¼š

```python
from bedrockx import read_file

# è¯»å–ä¸º list
data = read_file("data.jsonl")

# è¯»å–ä¸º dictï¼ˆä»¥ id ä¸ºé”®ï¼‰
data_dict = read_file("data.json", output_type="dict", main_key_column="id")

# è¯»å–ä¸º setï¼ˆåªä¿ç•™æŒ‡å®šåˆ—çš„å€¼ï¼‰
id_set = read_file("data.csv", output_type="set", main_key_column="id")
```

#### æ™ºèƒ½æ–‡ä»¶ä¿å­˜

è‡ªåŠ¨åˆ›å»ºç›®å½•ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š

```python
from bedrockx import save_file

# è‡ªåŠ¨æ ¹æ®åç¼€åä¿å­˜
save_file("output/result.jsonl", data)
save_file("output/result.xlsx", data)
save_file("output/result.csv", data)
```

#### è£…é¥°å™¨å¼æ–‡ä»¶è¿½åŠ 

è¾¹å¤„ç†è¾¹ä¿å­˜ï¼Œæ— éœ€ç¼“å­˜å¤§é‡æ•°æ®ï¼š

```python
from bedrockx import return_to_jsonl

@return_to_jsonl("results.jsonl")
def process_item(item):
    # å¤„ç†é€»è¾‘
    return {"id": item["id"], "result": item["value"] * 2}

for item in data:
    process_item(item)  # è‡ªåŠ¨è¿½åŠ åˆ°æ–‡ä»¶
```

### ğŸ”„ æ•°æ®å¤„ç†

#### æ•°æ®è¿‡æ»¤

```python
from bedrockx import filter_data

# è¿‡æ»¤æ‰å·²å¤„ç†çš„æ•°æ®
processed_ids = {1, 2, 3}
new_data = filter_data(data, processed_ids, main_key_column="id")
```

#### æ•°æ®å»é‡

```python
from bedrockx import drop_duplicates

# åŸºäº id å­—æ®µå»é‡
unique_data = drop_duplicates(data, main_key_column="id")
```

#### åˆ—åˆ é™¤

```python
from bedrockx import remove_columns

# åˆ é™¤æ•æ„Ÿå­—æ®µ
clean_data = remove_columns(data, ["password", "token"])
```

### âš¡ å¤šçº¿ç¨‹å¤„ç†

ä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿæ•°æ®å¤„ç†ï¼Œæ”¯æŒè¾¹å¤„ç†è¾¹ä¿å­˜ï¼š

```python
from bedrockx import BaseMultiThreading

class MyProcessor(BaseMultiThreading):
    def single_data_process(self, item):
        # å®šä¹‰å•ä¸ªæ•°æ®çš„å¤„ç†é€»è¾‘
        result = expensive_operation(item)
        return result

# ä½¿ç”¨ 4 ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†
processor = MyProcessor(max_workers=4, save_path="output.jsonl")
processor(data)  # è‡ªåŠ¨å¹¶å‘å¤„ç†å¹¶ä¿å­˜
```

### ğŸ› ï¸ å·¥å…·å‡½æ•°

#### å•ä¾‹æ¨¡å¼

```python
from bedrockx import singleton

@singleton
class Config:
    def __init__(self):
        self.settings = {}

# å…¨å±€å”¯ä¸€å®ä¾‹
config1 = Config()
config2 = Config()
assert config1 is config2  # True
```

#### æ—¥å¿—ç®¡ç†

```python
from bedrockx import LoggerManager

# åˆ›å»ºæ—¥å¿—ç®¡ç†å™¨
logger = LoggerManager("logs/app.log", level="INFO")

logger.info("ç¨‹åºå¯åŠ¨")
logger.warning("è­¦å‘Šä¿¡æ¯")
logger.error("é”™è¯¯ä¿¡æ¯")
```

---

## ğŸ“š è¯¦ç»†æ–‡æ¡£

### API å‚è€ƒ

#### `read_file()`

è¯»å–å„ç§æ ¼å¼çš„æ–‡ä»¶ã€‚

**å‚æ•°ï¼š**

- `file_name` (str | Path): æ–‡ä»¶è·¯å¾„
- `output_type` (str): è¿”å›ç±»å‹ï¼Œå¯é€‰ `"list"`, `"dict"`, `"set"`ï¼Œé»˜è®¤ `"list"`
- `file_type` (str): æ–‡ä»¶ç±»å‹ï¼Œå¯é€‰ `"json"`, `"jsonl"`, `"csv"`, `"xlsx"`ï¼Œé»˜è®¤è‡ªåŠ¨è¯†åˆ«
- `main_key_column` (str): å½“ `output_type="dict"` æˆ– `"set"` æ—¶ï¼ŒæŒ‡å®šç”¨ä½œé”®çš„å­—æ®µ
- `encoding` (str): æ–‡ä»¶ç¼–ç ï¼Œé»˜è®¤ `"utf-8"`
- `disable_tqdm` (bool): æ˜¯å¦ç¦ç”¨è¿›åº¦æ¡ï¼Œé»˜è®¤ `False`
- `**kwargs`: ä¼ é€’ç»™åº•å±‚è¯»å–å‡½æ•°çš„å…¶ä»–å‚æ•°

**è¿”å›ï¼š** list | dict | set

**ç¤ºä¾‹ï¼š**

```python
# åŸºæœ¬ç”¨æ³•
data = read_file("data.jsonl")

# è¯»å–ä¸ºå­—å…¸
data_dict = read_file(
    "data.json",
    output_type="dict",
    main_key_column="id"
)

# è¯»å– Excel ç‰¹å®š sheet
data = read_file(
    "data.xlsx",
    sheet_name="Sheet1"
)

# è¯»å– CSV æŒ‡å®šç¼–ç 
data = read_file(
    "data.csv",
    encoding="gbk"
)
```

#### `save_file()`

ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶ã€‚

**å‚æ•°ï¼š**

- `file_name` (str | Path): ä¿å­˜è·¯å¾„
- `data` (list): è¦ä¿å­˜çš„æ•°æ®
- `file_type` (str): æ–‡ä»¶ç±»å‹ï¼Œé»˜è®¤æ ¹æ®åç¼€è‡ªåŠ¨è¯†åˆ«
- `encoding` (str): æ–‡ä»¶ç¼–ç ï¼Œé»˜è®¤ `"utf-8"`
- `ensure_ascii` (bool): JSON æ ¼å¼æ˜¯å¦è½¬ä¹‰é ASCII å­—ç¬¦ï¼Œé»˜è®¤ `False`
- `json_indent` (int): JSON ç¼©è¿›ï¼Œé»˜è®¤ `4`
- `pd_index` (bool): DataFrame æ˜¯å¦ä¿å­˜ç´¢å¼•ï¼Œé»˜è®¤ `False`
- `**kwargs`: ä¼ é€’ç»™åº•å±‚ä¿å­˜å‡½æ•°çš„å…¶ä»–å‚æ•°

**ç¤ºä¾‹ï¼š**

```python
# åŸºæœ¬ç”¨æ³•
save_file("output.jsonl", data)

# ä¿å­˜ä¸ºå‹ç¼©æ ¼å¼çš„ JSON
save_file("output.json", data, json_indent=None)

# Excel ä¿å­˜ç‰¹å®š sheet
save_file("output.xlsx", data, sheet_name="Results")
```

#### `filter_data()`

è¿‡æ»¤æ•°æ®ã€‚

**å‚æ•°ï¼š**

- `data` (list[dict]): å¾…è¿‡æ»¤çš„æ•°æ®
- `filter_set` (set): è¦è¿‡æ»¤æ‰çš„å€¼çš„é›†åˆ
- `main_key_column` (str): ç”¨äºè¿‡æ»¤çš„å­—æ®µå

**è¿”å›ï¼š** list[dict]

**ç¤ºä¾‹ï¼š**

```python
# è¿‡æ»¤å·²å¤„ç†çš„ ID
processed_ids = {1, 2, 3, 4, 5}
new_data = filter_data(data, processed_ids, "id")
```

#### `drop_duplicates()`

å»é™¤é‡å¤æ•°æ®ã€‚

**å‚æ•°ï¼š**

- `data` (list[dict]): å¾…å»é‡çš„æ•°æ®
- `main_key_column` (str): ç”¨äºåˆ¤æ–­é‡å¤çš„å­—æ®µå

**è¿”å›ï¼š** list[dict]

**ç¤ºä¾‹ï¼š**

```python
# åŸºäº user_id å»é‡
unique_users = drop_duplicates(data, "user_id")
```

#### `remove_columns()`

åˆ é™¤æŒ‡å®šåˆ—ã€‚

**å‚æ•°ï¼š**

- `data` (list[dict]): æ•°æ®
- `key_list` (list | str): è¦åˆ é™¤çš„å­—æ®µåï¼ˆåˆ—è¡¨æˆ–å•ä¸ªå­—ç¬¦ä¸²ï¼‰

**è¿”å›ï¼š** list[dict]

**ç¤ºä¾‹ï¼š**

```python
# åˆ é™¤æ•æ„Ÿå­—æ®µ
clean_data = remove_columns(data, ["password", "email", "phone"])

# åˆ é™¤å•ä¸ªå­—æ®µ
clean_data = remove_columns(data, "temp_field")
```

#### `BaseMultiThreading`

å¤šçº¿ç¨‹å¤„ç†åŸºç±»ã€‚

**å‚æ•°ï¼š**

- `max_workers` (int): çº¿ç¨‹æ•°
- `save_path` (str | Path): ç»“æœä¿å­˜è·¯å¾„
- `file_type` (str): ä¿å­˜æ–‡ä»¶ç±»å‹ï¼Œé»˜è®¤æ ¹æ®åç¼€è‡ªåŠ¨è¯†åˆ«

**æ–¹æ³•ï¼š**

- `single_data_process(item: dict) -> dict`: éœ€è¦å­ç±»å®ç°ï¼Œå®šä¹‰å•ä¸ªæ•°æ®çš„å¤„ç†é€»è¾‘

**ç¤ºä¾‹ï¼š**

```python
from bedrockx import BaseMultiThreading
import time

class MyProcessor(BaseMultiThreading):
    def single_data_process(self, item):
        # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
        time.sleep(0.1)
        result = {
            "id": item["id"],
            "processed": True,
            "value": item["value"] * 2
        }
        return result

# ä½¿ç”¨ 10 ä¸ªçº¿ç¨‹å¤„ç†æ•°æ®
processor = MyProcessor(max_workers=10, save_path="results.jsonl")
processor(large_dataset)  # è‡ªåŠ¨å¹¶å‘å¤„ç†
```

#### `add_suffix_file()`

ä¸ºæ–‡ä»¶åæ·»åŠ åç¼€ã€‚

**å‚æ•°ï¼š**

- `file_path` (str | Path): åŸå§‹æ–‡ä»¶è·¯å¾„
- `suffix` (str): è¦æ·»åŠ çš„åç¼€
- `sep` (str): åˆ†éš”ç¬¦ï¼Œé»˜è®¤ `"_"`

**è¿”å›ï¼š** Path

**ç¤ºä¾‹ï¼š**

```python
from bedrockx import add_suffix_file

# ç”Ÿæˆå¸¦åç¼€çš„æ–‡ä»¶å
output_path = add_suffix_file("data.jsonl", "processed")
# ç»“æœ: Path("data_processed.jsonl")

# è‡ªå®šä¹‰åˆ†éš”ç¬¦
output_path = add_suffix_file("data.jsonl", "v2", sep="-")
# ç»“æœ: Path("data-v2.jsonl")
```

#### `@return_to_jsonl`

è£…é¥°å™¨ï¼Œè‡ªåŠ¨å°†å‡½æ•°è¿”å›å€¼è¿½åŠ åˆ° JSONL æ–‡ä»¶ã€‚

**å‚æ•°ï¼š**

- `file_path` (str | Path): ä¿å­˜è·¯å¾„
- `encoding` (str): æ–‡ä»¶ç¼–ç ï¼Œé»˜è®¤ `"utf-8"`
- `ensure_ascii` (bool): æ˜¯å¦è½¬ä¹‰é ASCII å­—ç¬¦ï¼Œé»˜è®¤ `False`

**ç¤ºä¾‹ï¼š**

```python
from bedrockx import return_to_jsonl

@return_to_jsonl("processed_data.jsonl")
def process_record(record):
    # å¤„ç†å•æ¡è®°å½•
    return {
        "id": record["id"],
        "result": some_computation(record)
    }

# æ‰¹é‡å¤„ç†ï¼Œè‡ªåŠ¨ä¿å­˜
for record in records:
    process_record(record)  # æ¯æ¬¡è°ƒç”¨éƒ½è¿½åŠ åˆ°æ–‡ä»¶
```

#### `@singleton`

å•ä¾‹æ¨¡å¼è£…é¥°å™¨ï¼Œç¡®ä¿ç±»åªæœ‰ä¸€ä¸ªå®ä¾‹ã€‚

**ç¤ºä¾‹ï¼š**

```python
from bedrockx import singleton

@singleton
class DatabaseConnection:
    def __init__(self):
        self.conn = create_connection()
    
    def query(self, sql):
        return self.conn.execute(sql)

# æ— è®ºåˆ›å»ºå¤šå°‘æ¬¡ï¼Œéƒ½æ˜¯åŒä¸€ä¸ªå®ä¾‹
db1 = DatabaseConnection()
db2 = DatabaseConnection()
assert db1 is db2  # True
```

#### `LoggerManager`

æ—¥å¿—ç®¡ç†å™¨ï¼ŒåŸºäº loguru çš„å°è£…ã€‚

**å‚æ•°ï¼š**

- `log_path` (str | None): æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œ`None` è¡¨ç¤ºä¸ä¿å­˜æ–‡ä»¶
- `level` (str): æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ `"INFO"`
- `rotation` (str): æ—¥å¿—è½®è½¬å¤§å°ï¼Œé»˜è®¤ `"10 MB"`
- `retention` (str): æ—¥å¿—ä¿ç•™æ—¶é—´ï¼Œé»˜è®¤ `"7 days"`
- `compression` (str): å‹ç¼©æ ¼å¼ï¼Œé»˜è®¤ `"zip"`
- `enqueue` (bool): æ˜¯å¦å¯ç”¨å¼‚æ­¥å†™å…¥ï¼Œé»˜è®¤ `True`
- `console` (bool): æ˜¯å¦è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œé»˜è®¤ `True`

**æ–¹æ³•ï¼š**

- `debug(msg)`: è°ƒè¯•ä¿¡æ¯
- `info(msg)`: ä¸€èˆ¬ä¿¡æ¯
- `warning(msg)`: è­¦å‘Šä¿¡æ¯
- `error(msg)`: é”™è¯¯ä¿¡æ¯
- `critical(msg)`: ä¸¥é‡é”™è¯¯
- `exception(msg)`: å¼‚å¸¸ä¿¡æ¯ï¼ˆå¸¦å †æ ˆï¼‰

**ç¤ºä¾‹ï¼š**

```python
from bedrockx import LoggerManager

# åˆ›å»ºæ—¥å¿—ç®¡ç†å™¨
logger = LoggerManager(
    log_path="logs/app.log",
    level="DEBUG",
    rotation="50 MB",
    retention="30 days"
)

logger.debug("è°ƒè¯•ä¿¡æ¯")
logger.info("ç¨‹åºå¯åŠ¨æˆåŠŸ")
logger.warning("é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥")

# è®°å½•å¼‚å¸¸
try:
    risky_operation()
except Exception as e:
    logger.exception("æ“ä½œå¤±è´¥")
```

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: æ•°æ®æ¸…æ´—æµç¨‹

```python
from bedrockx import read_file, save_file, drop_duplicates, remove_columns

# 1. è¯»å–åŸå§‹æ•°æ®
raw_data = read_file("raw_data.csv")
print(f"åŸå§‹æ•°æ®: {len(raw_data)} æ¡")

# 2. å»é‡
unique_data = drop_duplicates(raw_data, main_key_column="user_id")
print(f"å»é‡å: {len(unique_data)} æ¡")

# 3. åˆ é™¤ä¸éœ€è¦çš„å­—æ®µ
clean_data = remove_columns(unique_data, ["temp_col", "debug_info"])

# 4. ä¿å­˜æ¸…æ´—åçš„æ•°æ®
save_file("cleaned_data.jsonl", clean_data)
print("æ•°æ®æ¸…æ´—å®Œæˆï¼")
```

### ç¤ºä¾‹ 2: å¢é‡æ•°æ®å¤„ç†

```python
from bedrockx import read_file, save_file, filter_data

# è¯»å–å·²å¤„ç†çš„ ID
processed_ids = read_file(
    "processed.jsonl",
    output_type="set",
    main_key_column="id"
)

# è¯»å–æ–°æ•°æ®
new_data = read_file("new_batch.jsonl")

# è¿‡æ»¤å·²å¤„ç†çš„æ•°æ®
to_process = filter_data(new_data, processed_ids, main_key_column="id")
print(f"éœ€è¦å¤„ç†: {len(to_process)} æ¡æ–°æ•°æ®")

# å¤„ç†å¹¶ä¿å­˜...
```

### ç¤ºä¾‹ 3: å¤šçº¿ç¨‹ API è°ƒç”¨

```python
from bedrockx import BaseMultiThreading
import requests

class APIProcessor(BaseMultiThreading):
    def __init__(self, *args, api_key=None, **kwargs):
        self.api_key = api_key
        super().__init__(*args, **kwargs)
    
    def post_init(self, **kwargs):
        # åœ¨è¿™é‡Œå¯ä»¥åˆå§‹åŒ–å…¶ä»–èµ„æº
        self.api_key = kwargs.get('api_key')
    
    def single_data_process(self, item):
        # è°ƒç”¨ API å¤„ç†å•æ¡æ•°æ®
        response = requests.post(
            "https://api.example.com/process",
            json=item,
            headers={"Authorization": f"Bearer {self.api_key}"}
        )
        return response.json()

# ä½¿ç”¨ 20 ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†
processor = APIProcessor(
    max_workers=20,
    save_path="api_results.jsonl",
    api_key="your-api-key"
)

data = read_file("input_data.jsonl")
processor(data)  # è‡ªåŠ¨å¹¶å‘è°ƒç”¨ API å¹¶ä¿å­˜ç»“æœ
```

### ç¤ºä¾‹ 4: å®æ—¶æ•°æ®å¤„ç†

```python
from bedrockx import return_to_jsonl, LoggerManager

logger = LoggerManager("logs/processing.log")

@return_to_jsonl("processed_stream.jsonl")
def process_stream_data(data):
    try:
        # æ•°æ®è½¬æ¢
        result = {
            "timestamp": data["timestamp"],
            "value": data["raw_value"] * 1.5,
            "status": "processed"
        }
        logger.info(f"å¤„ç†æˆåŠŸ: {data['id']}")
        return result
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {data.get('id')}, é”™è¯¯: {e}")
        return None

# æ¨¡æ‹Ÿæµå¼æ•°æ®å¤„ç†
while True:
    data = receive_data()  # ä»æŸä¸ªæ•°æ®æºæ¥æ”¶
    process_stream_data(data)  # è‡ªåŠ¨ä¿å­˜åˆ°æ–‡ä»¶
```

### ç¤ºä¾‹ 5: æ•°æ®æ ¼å¼è½¬æ¢

```python
from bedrockx import read_file, save_file

# CSV è½¬ JSON
data = read_file("data.csv")
save_file("data.json", data)

# Excel è½¬ JSONL
data = read_file("data.xlsx", sheet_name="Sheet1")
save_file("data.jsonl", data)

# JSONL è½¬ Excel
data = read_file("data.jsonl")
save_file("data.xlsx", data)
```

### ç¤ºä¾‹ 6: é…ç½®ç®¡ç†

```python
from bedrockx import singleton
import json

@singleton
class Config:
    def __init__(self, config_path="config.json"):
        with open(config_path) as f:
            self.settings = json.load(f)
    
    def get(self, key, default=None):
        return self.settings.get(key, default)

# åœ¨ä»»ä½•åœ°æ–¹éƒ½å¯ä»¥è·å–åŒä¸€ä¸ªé…ç½®å®ä¾‹
config = Config()
api_key = config.get("api_key")
```

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æ•°æ®å¤„ç†ç®¡é“

```python
from bedrockx import (
    read_file, save_file, filter_data, 
    drop_duplicates, remove_columns, LoggerManager
)

class DataPipeline:
    def __init__(self, log_path="logs/pipeline.log"):
        self.logger = LoggerManager(log_path)
    
    def run(self, input_file, output_file, processed_ids_file=None):
        # è¯»å–æ•°æ®
        self.logger.info(f"è¯»å–æ•°æ®: {input_file}")
        data = read_file(input_file)
        self.logger.info(f"è¯»å–å®Œæˆ: {len(data)} æ¡")
        
        # å»é‡
        data = drop_duplicates(data, "id")
        self.logger.info(f"å»é‡å: {len(data)} æ¡")
        
        # è¿‡æ»¤å·²å¤„ç†æ•°æ®
        if processed_ids_file:
            processed = read_file(
                processed_ids_file,
                output_type="set",
                main_key_column="id"
            )
            data = filter_data(data, processed, "id")
            self.logger.info(f"è¿‡æ»¤å: {len(data)} æ¡")
        
        # åˆ é™¤ä¸´æ—¶å­—æ®µ
        data = remove_columns(data, ["_temp", "_debug"])
        
        # ä¿å­˜ç»“æœ
        save_file(output_file, data)
        self.logger.info(f"ä¿å­˜å®Œæˆ: {output_file}")
        
        return data

# ä½¿ç”¨ç®¡é“
pipeline = DataPipeline()
pipeline.run("input.jsonl", "output.jsonl", "processed.jsonl")
```

### æ‰©å±• BaseMultiThreading

```python
from bedrockx import BaseMultiThreading, LoggerManager
import time

class RetryableProcessor(BaseMultiThreading):
    def post_init(self, max_retries=3, **kwargs):
        self.max_retries = max_retries
        self.logger = LoggerManager("logs/processor.log")
    
    def single_data_process(self, item):
        for attempt in range(self.max_retries):
            try:
                result = self.process_with_retry(item)
                return result
            except Exception as e:
                self.logger.warning(
                    f"å¤„ç†å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}"
                )
                if attempt == self.max_retries - 1:
                    self.logger.error(f"æœ€ç»ˆå¤±è´¥: {item.get('id')}")
                    raise
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
    
    def process_with_retry(self, item):
        # å®é™…çš„å¤„ç†é€»è¾‘
        raise NotImplementedError

# ä½¿ç”¨
class MyRetryProcessor(RetryableProcessor):
    def process_with_retry(self, item):
        # å¯èƒ½å¤±è´¥çš„æ“ä½œ
        return call_unreliable_api(item)

processor = MyRetryProcessor(
    max_workers=5,
    save_path="output.jsonl",
    max_retries=5
)
```

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ–°åŠŸèƒ½å»ºè®®ï¼

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/ciaoyizhen/bedrockx.git
cd bedrockx

# å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# è¿è¡Œæµ‹è¯•
pytest tests/

# æ£€æŸ¥ä»£ç è¦†ç›–ç‡
pytest tests/ --cov=bedrockx --cov-report=html
```

### æäº¤ PR çš„æ­¥éª¤

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

### ä»£ç è§„èŒƒ

- éµå¾ª PEP 8 ä»£ç é£æ ¼
- æ·»åŠ ç±»å‹æ³¨è§£
- ç¼–å†™å®Œæ•´çš„æ–‡æ¡£å­—ç¬¦ä¸²
- ä¸ºæ–°åŠŸèƒ½æ·»åŠ æµ‹è¯•

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

---

## ğŸ™ è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…å’Œä½¿ç”¨ bedrockx çš„å¼€å‘è€…ï¼

ç‰¹åˆ«æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®ï¼š

- [pandas](https://pandas.pydata.org/) - æ•°æ®å¤„ç†
- [loguru](https://github.com/Delgan/loguru) - æ—¥å¿—ç®¡ç†
- [tqdm](https://github.com/tqdm/tqdm) - è¿›åº¦æ¡

---

## ğŸ“ è”ç³»æ–¹å¼

- **ä½œè€…**: ciaoyizhen
- **é‚®ç®±**: yizhen.ciao@gmail.com
- **GitHub**: [@ciaoyizhen](https://github.com/ciaoyizhen)
- **Issue Tracker**: [GitHub Issues](https://github.com/ciaoyizhen/bedrockx/issues)


<div align="center">

**[â¬† è¿”å›é¡¶éƒ¨](#bedrockx)**

Made with â¤ï¸ by ciaoyizhen

</div>